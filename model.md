# Model

1. 尝试简单的逻辑斯蒂回归
2. 尝试决策树/随机森林等经典机器学习算法
3. 神经网络（多层感知机）
4. XGBoost/LightGBM - 当作基准

- 准确率：（（实际阳性，检测阳性）+ （实际阴性，检测阴性））/ 总人数
- 精确率：(实际阳性，检测阳性) / 检测阳性
- 召回率：（实际阳性，检测阳性）/ 实际阳性
- F1分数： 召回率和精确率的调和平均

## Phase 1

尝试了贴合比较好的几个数据进行逻辑回归和多层感知机训练，总体结果非常不好，训练损失几乎不下降，学习效果也很一般，准确率只有60%上下

这里可以看一下MLP的损失变化

- 学习率较小的时候：

![mlp学习率较低的时候](mlp_1.png)

```text
平均损失: 0.6277
精确率: 0.6452
召回率: 1.0000
F1分数: 0.7843
准确率: 0.6298
```

- 学习率再大一点：

![mlp学习率大一点](mlp_2.png)

```text
测试结果(设备: cuda): 
平均损失: 0.6625
精确率: 0.6250
召回率: 1.0000
F1分数: 0.7692
准确率: 0.6233
```

逻辑回归也不用看了，就基本不下降的, 我们也尝试了很多的调参，最后也没有给调出一个好的结果，基本是无法学习。

原因推测是因为我们的数据本身非常离散化，不利于深度学习进行反向传播和梯度下降

## Phase2

对随机森林的超参数进行了筛选

```text
n_estimators  max_depth  accuracy  precision    recall  f1_score
         100          5  0.628321   0.634315  0.953221  0.761737
         100         10  0.633564   0.645816  0.912584  0.756367
         100         15  0.634121   0.648275  0.902820  0.754661
         100         20  0.624707   0.651156  0.857015  0.740036
         100         25  0.605243   0.653877  0.779031  0.710988
         200          5  0.628457   0.634401  0.953255  0.761810
         200         10  0.633357   0.645858  0.911644  0.756073
         200         15  0.634200   0.648146  0.903703  0.754882
         200         20  0.624764   0.650881  0.858413  0.740379
         200         25  0.606536   0.654227  0.782079  0.712462
         300          5  0.628407   0.634289  0.953691  0.761868
         300         10  0.633007   0.645665  0.911335  0.755835
         300         15  0.634014   0.647959  0.903932  0.754835
         300         20  0.624921   0.650754  0.859513  0.740706
         300         25  0.607071   0.654431  0.783110  0.713011
         400          5  0.628314   0.634222  0.953714  0.761827
         400         10  0.633236   0.645667  0.912137  0.756112
         400         15  0.633943   0.647906  0.903932  0.754799
         400         20  0.624950   0.650656  0.860041  0.740838
         400         25  0.607221   0.654420  0.783661  0.713233
         500          5  0.628264   0.634166  0.953840  0.761827
         500         10  0.633321   0.645622  0.912664  0.756261
         500         15  0.634021   0.647962  0.903943  0.754841
         500         20  0.624757   0.650439  0.860327  0.740803
         500         25  0.607807   0.654679  0.784646  0.713795
```

1. max_depth = 5 那一组各项指标几乎原地不动，且 recall 最高（≈0.954），F1 也最高（≈0.762）。
2. 随着 depth 增大，precision 微升，但 recall 明显下降，F1 被拉低。
3. n_estimators 从 100 加到 500，指标基本 不再变化，说明树数已饱和。

选择n_estimator=200, max_depth=5 

尝试了更多的模型

| Model             | Accuracy | Precision | Recall  | F1 Score |
|-------------------|----------|-----------|---------|----------|
| Random Forest     | 0.6285   | 0.6344    | 0.9533  | 0.7618   |
| Logistic Regression| 0.6293  | 0.6414    | 0.9190  | 0.7555   |
| SVM               | 0.6233   | 0.6233    | 1.0000  | 0.7679   |
| KNN               | 0.5875   | 0.6522    | 0.7247  | 0.6865   |
| Decision Tree     | 0.5560   | 0.6493    | 0.6255  | 0.6372   |
| Naive Bayes       | 0.5761   | 0.7219    | 0.5203  | 0.6048   |
| Neural Network    | 0.6308   | 0.6429    | 0.9170  | 0.7559   |

初步分析可以发现，对于召回率而言，只有随机森林，神经网络，逻辑回归和有限向量机成功达到了90%以上，而有限向量机更是意外地达到了100%的召回率！  

0.6233 positive samples in training set.
0.6233 positive samples in test set.

> 这是模型很好吗？不，这单纯是因为一共有62.33%的数据是阳性的，只要每一个都预测是阳性，就能达到这个准确率和召回率

## KNN

对K邻居数进行了一定的筛选

| n_neighbors | Accuracy | Precision | Recall | F1 Score | Correct Probability Mean | Convince Probability Mean |
|-------------|----------|-----------|--------|----------|--------------------------|---------------------------|
| 5           | 0.5821   | 0.6530    | 0.7031 | 0.6771   | 0.5610                   | 0.7658                    |
| 15          | 0.5980   | 0.6529    | 0.7578 | 0.7015   | 0.5609                   | 0.7133                    |
| 25          | 0.6054   | 0.6530    | 0.7829 | 0.7121   | 0.5608                   | 0.6995                    |
| 35          | 0.6092   | 0.6524    | 0.7984 | 0.7181   | 0.5610                   | 0.6929                    |
| 45          | 0.6121   | 0.6519    | 0.8106 | 0.7226   | 0.5611                   | 0.6889                    |
| 55          | 0.6142   | 0.6515    | 0.8194 | 0.7259   | 0.5610                   | 0.6862                    |
| 65          | 0.6160   | 0.6513    | 0.8261 | 0.7284   | 0.5611                   | 0.6843                    |
| 75          | 0.6169   | 0.6509    | 0.8312 | 0.7301   | 0.5611                   | 0.6828                    |
| 85          | 0.6177   | 0.6505    | 0.8356 | 0.7315   | 0.5611                   | 0.6816                    |
| 95          | 0.6185   | 0.6502    | 0.8396 | 0.7329   | 0.5611                   | 0.6807                    |
| 105         | 0.6192   | 0.6501    | 0.8428 | 0.7340   | 0.5610                   | 0.6799                    |
| 115         | 0.6198   | 0.6499    | 0.8452 | 0.7348   | 0.5610                   | 0.6792                    |
| 125         | 0.6204   | 0.6499    | 0.8477 | 0.7357   | 0.5610                   | 0.6787                    |
| 135         | 0.6207   | 0.6497    | 0.8495 | 0.7363   | 0.5610                   | 0.6782                    |
| 145         | 0.6214   | 0.6498    | 0.8515 | 0.7371   | 0.5610                   | 0.6778                    |
| 155         | 0.6214   | 0.6495    | 0.8526 | 0.7374   | 0.5610                   | 0.6775                    |
| 165         | 0.6220   | 0.6496    | 0.8546 | 0.7381   | 0.5610                   | 0.6772                    |
| 175         | 0.6221   | 0.6494    | 0.8557 | 0.7384   | 0.5610                   | 0.6769                    |
| 185         | 0.6223   | 0.6493    | 0.8570 | 0.7388   | 0.5610                   | 0.6767                    |
| 195         | 0.6223   | 0.6490    | 0.8581 | 0.7390   | 0.5610                   | 0.6764                    |
| 205         | 0.6221   | 0.6487    | 0.8588 | 0.7391   | 0.5609                   | 0.6762                    |
| 215         | 0.6225   | 0.6488    | 0.8597 | 0.7395   | 0.5609                   | 0.6761                    |
| 225         | 0.6227   | 0.6487    | 0.8608 | 0.7399   | 0.5609                   | 0.6759                    |
| 235         | 0.6226   | 0.6486    | 0.8612 | 0.7399   | 0.5609                   | 0.6757                    |
| 245         | 0.6231   | 0.6487    | 0.8622 | 0.7404   | 0.5609                   | 0.6756                    |
| 255         | 0.6233   | 0.6487    | 0.8631 | 0.7407   | 0.5609                   | 0.6754                    |
| 265         | 0.6232   | 0.6484    | 0.8638 | 0.7408   | 0.5609                   | 0.6753                    |
| 275         | 0.6231   | 0.6483    | 0.8639 | 0.7407   | 0.5609                   | 0.6752                    |
| 285         | 0.6233   | 0.6483    | 0.8646 | 0.7410   | 0.5609                   | 0.6751                    |
| 295         | 0.6234   | 0.6483    | 0.8650 | 0.7411   | 0.5608                   | 0.6750                    |

---

## Decision Tree

## 规划

准备`XGBoost`, `LightGBM`的部署，并且学习其他参赛者的模型
