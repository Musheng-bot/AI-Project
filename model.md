# Model

1. 尝试简单的逻辑斯蒂回归
2. 尝试决策树/随机森林等经典机器学习算法
3. 神经网络（多层感知机）
4. XGBoost/LightGBM - 当作基准

## Phase 1

尝试了贴合比较好的几个数据进行逻辑回归和多层感知机训练，总体结果非常不好，训练损失几乎不下降，学习效果也很一般，准确率只有60%上下

这里可以看一下MLP的损失变化

- 学习率较小的时候：

![mlp学习率较低的时候](mlp_1.png)

```text
平均损失: 0.6277
精确率: 0.6452
召回率: 1.0000
F1分数: 0.7843
准确率: 0.6298
```

- 学习率再大一点：

![mlp学习率大一点](mlp_2.png)

```text
测试结果(设备: cuda): 
平均损失: 0.6625
精确率: 0.6250
召回率: 1.0000
F1分数: 0.7692
准确率: 0.6233
```

逻辑回归也不用看了，就基本不下降的, 我们也尝试了很多的调参，最后也没有给调出一个好的结果，基本是无法学习。

原因推测是因为我们的数据本身非常离散化，不利于深度学习进行反向传播和梯度下降

## Phase2

对随机森林的超参数进行了筛选

```text
n_estimators  max_depth  accuracy  precision    recall  f1_score
         100          5  0.628321   0.634315  0.953221  0.761737
         100         10  0.633564   0.645816  0.912584  0.756367
         100         15  0.634121   0.648275  0.902820  0.754661
         100         20  0.624707   0.651156  0.857015  0.740036
         100         25  0.605243   0.653877  0.779031  0.710988
         200          5  0.628457   0.634401  0.953255  0.761810
         200         10  0.633357   0.645858  0.911644  0.756073
         200         15  0.634200   0.648146  0.903703  0.754882
         200         20  0.624764   0.650881  0.858413  0.740379
         200         25  0.606536   0.654227  0.782079  0.712462
         300          5  0.628407   0.634289  0.953691  0.761868
         300         10  0.633007   0.645665  0.911335  0.755835
         300         15  0.634014   0.647959  0.903932  0.754835
         300         20  0.624921   0.650754  0.859513  0.740706
         300         25  0.607071   0.654431  0.783110  0.713011
         400          5  0.628314   0.634222  0.953714  0.761827
         400         10  0.633236   0.645667  0.912137  0.756112
         400         15  0.633943   0.647906  0.903932  0.754799
         400         20  0.624950   0.650656  0.860041  0.740838
         400         25  0.607221   0.654420  0.783661  0.713233
         500          5  0.628264   0.634166  0.953840  0.761827
         500         10  0.633321   0.645622  0.912664  0.756261
         500         15  0.634021   0.647962  0.903943  0.754841
         500         20  0.624757   0.650439  0.860327  0.740803
         500         25  0.607807   0.654679  0.784646  0.713795
```

1. max_depth = 5 那一组各项指标几乎原地不动，且 recall 最高（≈0.954），F1 也最高（≈0.762）。
2. 随着 depth 增大，precision 微升，但 recall 明显下降，F1 被拉低。
3. n_estimators 从 100 加到 500，指标基本 不再变化，说明树数已饱和。

选择n_estimator=200, max_depth=5 

尝试了更多的模型

| Model             | Accuracy | Precision | Recall  | F1 Score |
|-------------------|----------|-----------|---------|----------|
| Random Forest     | 0.6285   | 0.6344    | 0.9533  | 0.7618   |
| Logistic Regression| 0.6293  | 0.6414    | 0.9190  | 0.7555   |
| SVM               | 0.6233   | 0.6233    | 1.0000  | 0.7679   |
| KNN               | 0.5875   | 0.6522    | 0.7247  | 0.6865   |
| Decision Tree     | 0.5560   | 0.6493    | 0.6255  | 0.6372   |
| Naive Bayes       | 0.5761   | 0.7219    | 0.5203  | 0.6048   |
| Neural Network    | 0.6308   | 0.6429    | 0.9170  | 0.7559   |
