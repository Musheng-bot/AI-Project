# Model

1. 尝试简单的逻辑斯蒂回归
2. 尝试决策树/随机森林等经典机器学习算法
3. 神经网络（多层感知机）
4. XGBoost/LightGBM - 当作基准

## Phase 1

尝试了贴合比较好的几个数据进行逻辑回归和多层感知机训练，总体结果非常不好，训练损失几乎不下降，学习效果也很一般，准确率只有60%上下

这里可以看一下MLP的损失变化

- 学习率较小的时候：

![mlp学习率较低的时候](mlp_1.png)

```text
平均损失: 0.6277
精确率: 0.6452
召回率: 1.0000
F1分数: 0.7843
准确率: 0.6298
```

- 学习率再大一点：

![mlp学习率大一点](mlp_2.png)

```text
测试结果(设备: cuda): 
平均损失: 0.6625
精确率: 0.6250
召回率: 1.0000
F1分数: 0.7692
准确率: 0.6233
```

逻辑回归也不用看了，就基本不下降的, 我们也尝试了很多的调参，最后也没有给调出一个好的结果，基本是无法学习。

原因推测是因为我们的数据本身非常离散化，不利于深度学习进行反向传播和梯度下降

## Phase2
